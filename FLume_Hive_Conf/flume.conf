
logAnalyzeAG.sources = r1  
logAnalyzeAG.channels = c1  
logAnalyzeAG.sinks = k1  
  

logAnalyzeAG.sources.r1.type = spooldir  
logAnalyzeAG.sources.r1.spoolDir = /home/hadoop/Documents/log


logAnalyzeAG.sources.s1.deserializer.maxLineLength =1048576  
logAnalyzeAG.sources.s1.fileSuffix = .DONE  
logAnalyzeAG.sources.s1.ignorePattern = access(_\d{4}\-\d{2}\-\d{2}_\d{2})?\.log(\.DONE)?  
logAnalyzeAG.sources.s1.consumeOrder = oldest  
logAnalyzeAG.sources.s1.deserializer = org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder  
logAnalyzeAG.sources.s1.batchsize = 5  

#logAnalyzeAG.sources.r1.interceptors = i1  
#logAnalyzeAG.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder  
  
  

logAnalyzeAG.channels.c1.type = memory  
logAnalyzeAG.channels.c1.capacity = 10000  
logAnalyzeAG.channels.c1.transactionCapacity = 100  
  

logAnalyzeAG.sinks.k1.type = hdfs  
#%y-%m-%d/%H%M/%S  

logAnalyzeAG.sinks.k1.hdfs.path = hdfs://localhost:9000/hive/warehouse/hive_1208.db/td_log_analyze/%Y-%m-%d_%H  
logAnalyzeAG.sinks.k1.hdfs.filePrefix = nginx-%Y-%m-%d_%H  
logAnalyzeAG.sinks.k1.hdfs.fileSuffix = .log  
logAnalyzeAG.sinks.k1.hdfs.fileType = DataStream  
logAnalyzeAG.sinks.k1.hdfs.callTimeout = 180000
 
logAnalyzeAG.sinks.k1.hdfs.rollCount = 0  

logAnalyzeAG.sinks.k1.hdfs.rollSize = 2914560  

#logAnalyzeAG.sinks.k1.hdfs.rollInterval = 60  
logAnalyzeAG.sinks.k1.hdfs.useLocalTimeStamp = true  
  

logAnalyzeAG.sources.r1.channels = c1  
logAnalyzeAG.sinks.k1.channel = c1  


------------------------------------------------------------------------------------------------------------------------------------------------------

# 启动 Flume

./bin/flume-ng agent -c conf -f conf/hive_1208.conf -n logAnalyzeAG -Dflume.root.logger=INFO,console












